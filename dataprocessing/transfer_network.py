import re
import csv

def parse_network_file(filepath):
    with open(filepath, 'r') as f:
        lines = f.readlines()

    records = []
    first_timestamp = None
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith("# Time:"):
            # æå–æ—¶é—´æˆ³
            time_match = re.search(r'# Time:(\d+\.\d+)', line)
            if not time_match:
                i += 1
                continue
            timestamp = float(time_match.group(1))
            if first_timestamp is None:
                first_timestamp = timestamp
            i += 1
            # å¤„ç†è¯¥æ—¶é—´ç‚¹ä¸‹çš„æ‰€æœ‰IP
            while i < len(lines) and not lines[i].strip().startswith("# Time"):
                ip_line = lines[i].strip()
                if ip_line.startswith("# IP:"):
                    ip_match = re.match(r'# IP:\s*([\d\.]+)', ip_line)
                    if ip_match:
                        ip = ip_match.group(1)
                        metrics = {}
                        i += 1
                        # è¯»å–è¯¥IPä¸‹çš„æ‰€æœ‰æŒ‡æ ‡è¡Œ
                        while i < len(lines) and lines[i].strip() and not lines[i].strip().startswith("# IP") and not lines[i].strip().startswith("# Time"):
                            metric_line = lines[i].strip()
                            metric_match = re.match(r'(\w+):\s*(\d+)', metric_line)
                            if metric_match:
                                metrics[metric_match.group(1)] = int(metric_match.group(2))
                            i += 1
                        record = {'Time': timestamp, 'IP': ip}
                        record.update(metrics)
                        elapsed = timestamp - first_timestamp
                        normal_duration = 123.00
                        record['target'] = 0 if elapsed < normal_duration else 1
                        records.append(record)
                    else:
                        i += 1
                else:
                    i += 1
        else:
            i += 1
    return records

def save_to_csv(records, output_path):
    if not records:
        raise ValueError("æœªè§£æåˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")

    # è·å–æ‰€æœ‰å­—æ®µåï¼ˆä¿æŒ Time åœ¨å‰ï¼Œtarget åœ¨åï¼‰
    fieldnames = ['Time']
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡åï¼ˆæ’é™¤ Time å’Œ targetï¼‰
    metric_keys = set()
    for rec in records:
        for k in rec.keys():
            if k not in ('Time', 'target'):
                metric_keys.add(k)
    fieldnames += sorted(metric_keys)  # æˆ–æŒ‰å‡ºç°é¡ºåº
    fieldnames.append('target')

    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, restval='')
        writer.writeheader()
        for row in records:
            # è¡¥å…¨ç¼ºå¤±å­—æ®µï¼ˆç†è®ºä¸Šä¸ä¼šç¼ºï¼‰
            full_row = {k: row.get(k, '') for k in fieldnames}
            writer.writerow(full_row)

def parse_normal_intervals(config_str):
    intervals = []
    current_time = 0
    # åˆ†å‰²å­—ç¬¦ä¸² "2:600,200:10..."
    parts = config_str.strip().split(',')
    
    for part in parts:
        if not part: continue
        # è§£æ "è¯·æ±‚æ•°:æŒç»­æ—¶é—´"
        req_count, duration = map(int, part.split(':'))
        
        end_time = current_time + duration
        
        # å¦‚æœè¯·æ±‚æ•°ä¸º 1 æˆ– 2ï¼Œåˆ™è§†ä¸ºæ­£å¸¸é˜¶æ®µ
        if req_count in [1, 2]:
            intervals.append([current_time, end_time])
            
        current_time = end_time
    
    return intervals

config_data = "2:600,200:10,1:300,100:15,1:450,150:10,2:600"
normal_duration = parse_normal_intervals(config_data)

# è°ƒæ•´ CSV æ–‡ä»¶ä¸­çš„ target åˆ—, normal_durationä¸ºæ­£å¸¸åŒºé—´åˆ—è¡¨ï¼Œå°†æ­£å¸¸åŒºé—´å†…çš„ target è®¾ä¸º 0ï¼Œå¼‚å¸¸åŒºé—´çš„targetè®¾ä¸º1
def target_adjustment_duration(filepath, output_path, normal_duration):
    with open(filepath, 'r') as f:
        reader = csv.DictReader(f)
        records = list(reader)
    first_timestamp = None
    for row in records:
        timestamp = float(row['Time'])
        if first_timestamp is None:
            first_timestamp = timestamp
        # åˆ¤æ–­ timestamp æ˜¯å¦åœ¨ä»»ä½•æ­£å¸¸åŒºé—´å†…
        is_normal = any(start <= (timestamp-first_timestamp) < end for start, end in normal_duration)
        row['target'] = 0 if is_normal else 1
    # ä¿å­˜è°ƒæ•´åçš„æ–‡ä»¶
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = records[0].keys()
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

def target_adjustment_rxpackets(filepath, output_path, threshold_h,threshold_l):
    with open(filepath, 'r') as f:
        reader = csv.DictReader(f)
        records = list(reader)
    pri_02 = 0
    pri_03 = 0
    for row in records:
        '''
        if row['IP']=='192.168.122.102':
            if pri_02 ==0:
                pri_02 = int(row['rx_packets'])
                row['target'] = 0
            elif abs(int(row['rx_packets']) - pri_02)/int(row['rx_packets']) > 0.9 or int(row['rx_packets']) <50:
                row['target'] = 1
                pri_02 = int(row['rx_packets'])
            else:
                row['target'] = 0
        else:
            if row['IP']=='192.168.122.103':
                if pri_03 ==0:
                    pri_03 = int(row['rx_packets'])
                    row['target'] = 0
                elif abs(int(row['rx_packets']) - pri_03)/int(row['rx_packets']) > 0.9 or int(row['rx_packets']) <50:
                    row['target'] = 1
                    pri_03 = int(row['rx_packets'])
                else:
                    row['target'] = 0
        '''
        if row['IP']=='192.168.122.102':
            if int(row['rx_packets']) > threshold_l or int(row['rx_packets']) <50:
                row['target'] = 1
            else:
                row['target'] = 0
        else:
            if row['IP']=='192.168.122.103':
                if int(row['rx_packets']) > threshold_h or int(row['rx_packets']) <50:
                    row['target'] = 1
                else:
                    row['target'] = 0
    # ä¿å­˜è°ƒæ•´åçš„æ–‡ä»¶
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = records[0].keys()
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

def target_adjustment_txbytes(filepath, output_path, threshold=50000):
    with open(filepath, 'r') as f:
        reader = csv.DictReader(f)
        records = list(reader)
    for row in records:
        if int(row['tx_bytes']) > threshold:
            row['target'] = 1
        else:
            row['target'] = 0
    # ä¿å­˜è°ƒæ•´åçš„æ–‡ä»¶
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = records[0].keys()
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
input_file = "/workspace/lyc/abnormal_data/network-error/collapse_caused_by_speed/network-1765122029.9178896"      # æ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶å
output_file = "/workspace/gpu_cluster/data_processing/4090/network/network_metrics_labeled.csv"

records = parse_network_file(input_file)
save_to_csv(records, output_file)
# target_adjustment_rxpackets(output_file, output_file, threshold=300) # for burst
# target_adjustment_txbytes(output_file, output_file, threshold=50000) # for oom
target_adjustment_rxpackets(output_file, output_file, threshold_h=300000, threshold_l=8000) # for 4090 oom
print(f"âœ… æˆåŠŸè§£æ {len(records)} ä¸ªæ—¶é—´ç‚¹")
print(f"ğŸ’¾ å·²ä¿å­˜å¸¦æ ‡ç­¾çš„ CSV æ–‡ä»¶ï¼š{output_file}")

# å¯é€‰ï¼šæ‰“å°å‰å‡ è¡ŒéªŒè¯
for r in records[:2]:
    print(r)